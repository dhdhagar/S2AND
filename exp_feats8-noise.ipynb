{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891d7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74cbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc813d64",
   "metadata": {},
   "source": [
    "# Get best runs from sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a89b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-MLP runs: 131\n",
      "MLP runs: 46\n",
      "Total runs: 177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ac6ac5096043ab97efdb01bf06db68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7538439cc4764a60aefbcace122307cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best runs found for 111 sweeps\n",
      "Total finished runs across sweeps = 186\n"
     ]
    }
   ],
   "source": [
    "# List of tags to identify the set of sweeps in this analysis\n",
    "\n",
    "IN_TAGS = [\"feats8-noise1\", \"feats8-noise2\", \"feats8-noise3\"]\n",
    "\n",
    "runs = api.runs(\"dhdhagar/prob-ent-resolution\",\n",
    "                filters={\"tags\": {\"$in\": IN_TAGS}, \n",
    "                         \"state\": {\"$eq\": \"finished\"},\n",
    "                         \"config.pairwise_mode\": {\"$eq\": False}\n",
    "                        },\n",
    "                order=\"-summary_metrics.best_dev_b3_f1\"\n",
    "               )\n",
    "print(f\"Non-MLP runs: {len(runs)}\")\n",
    "runs_mlp = api.runs(\"dhdhagar/prob-ent-resolution\", \n",
    "                filters={\"tags\": {\"$in\": IN_TAGS}, \n",
    "                         \"state\": {\"$eq\": \"finished\"},\n",
    "                         \"config.pairwise_mode\": {\"$eq\": True}\n",
    "                        },\n",
    "                order=\"-summary_metrics.best_dev_auroc\"\n",
    "               )\n",
    "print(f\"MLP runs: {len(runs_mlp)}\")\n",
    "print(f\"Total runs: {len(runs) + len(runs_mlp)}\")\n",
    "\n",
    "methods = {'e2e', 'e2e-nosdp', 'frac', 'frac-nosdp', 'mlp'}\n",
    "\n",
    "def make_key(noise, model, dataset, seed):\n",
    "    return f\"noise{noise}_{model}_{dataset}_{seed}\"\n",
    "def run_key(run):\n",
    "    dataset = run._attrs['config']['dataset']\n",
    "    dataset_seed = run._attrs['config']['dataset_random_seed']\n",
    "    method = set(run.tags).intersection(methods).pop()\n",
    "    noise = run._attrs['config']['noise_std']\n",
    "    key = make_key(noise, method, dataset, dataset_seed)\n",
    "    return key\n",
    "def details_from_key(key):\n",
    "    noise, model, dataset, seed = key.split('_')\n",
    "    noise = noise[-1]\n",
    "    return noise, model, dataset, seed\n",
    "\n",
    "best_runs = {}\n",
    "finished_runs = defaultdict(int)\n",
    "sweep_ids = {}\n",
    "for _runs in [runs, runs_mlp]:\n",
    "    for run in tqdm(_runs):\n",
    "        if len(dict(run.summary)) < 10:\n",
    "            continue\n",
    "        key = run_key(run)\n",
    "        finished_runs[key] += 1\n",
    "        if key in best_runs:\n",
    "            continue\n",
    "        best_runs[key] = run\n",
    "        sweep_ids[key] = run.sweepName\n",
    "        \n",
    "print(f\"Best runs found for {len(best_runs)} sweeps\")\n",
    "print(f\"Total finished runs across sweeps = {sum(finished_runs.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1e97a",
   "metadata": {},
   "source": [
    "# Manage sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d36ca561",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SWEEP_PREFIX=\"feats8-noise\"\n",
    "_SWEEP_FEATS8_FLAGS = \"--keep_feat_idxs=0 --keep_feat_idxs=1 --keep_feat_idxs=2 \\\n",
    "    --keep_feat_idxs=3 --keep_feat_idxs=4 --keep_feat_idxs=5 \\\n",
    "    --keep_feat_idxs=14 --keep_feat_idxs=15\"\n",
    "\n",
    "# Add agents to sweeps with fewer completed runs\n",
    "\n",
    "def add_agents(max_run_count, launched, finished_runs, sweep_ids, n_agents=1, partition=\"cpu\"):\n",
    "    for k, v in finished_runs.items():\n",
    "        if k in launched:\n",
    "            continue\n",
    "        if v < max_run_count:\n",
    "            noise, model, dataset, seed = details_from_key(k)\n",
    "            sweep_id = sweep_ids[k]\n",
    "            SWEEP_FLAGS = f\"{_SWEEP_FEATS8_FLAGS} --noise_std={noise}\"\n",
    "            SWEEP_PREFIX = f\"{_SWEEP_PREFIX}{noise}\"\n",
    "            print(f'./add_agent.sh {dataset} {seed} {model} dhdhagar/prob-ent-resolution/{sweep_id} {n_agents} {partition} \"{SWEEP_FLAGS}\" {SWEEP_PREFIX}')\n",
    "            !./add_agent.sh $dataset $seed $model dhdhagar/prob-ent-resolution/$sweep_id $n_agents $partition \"$SWEEP_FLAGS\" $SWEEP_PREFIX\n",
    "            launched.add(k)\n",
    "\n",
    "def add_agents_by_list(keys_to_add, launched, sweep_ids, n_agents=1, partition=\"cpu\"):\n",
    "    for k in keys_to_add:\n",
    "        noise, model, dataset, seed = details_from_key(k)\n",
    "        sweep_id = sweep_ids[k]\n",
    "        SWEEP_FLAGS = f\"{_SWEEP_FEATS8_FLAGS} --noise_std={noise}\"\n",
    "        SWEEP_PREFIX = f\"{_SWEEP_PREFIX}{noise}\"\n",
    "        print(f'./add_agent.sh {dataset} {seed} {model} dhdhagar/prob-ent-resolution/{sweep_id} {n_agents} {partition} \"{SWEEP_FLAGS}\" {SWEEP_PREFIX}')\n",
    "        !./add_agent.sh $dataset $seed $model dhdhagar/prob-ent-resolution/$sweep_id $n_agents $partition \"$SWEEP_FLAGS\" $SWEEP_PREFIX\n",
    "        launched.add(k)\n",
    "\n",
    "def add_sweeps_by_list(keys_to_add, launched, partition=\"cpu\"):\n",
    "    for k in keys_to_add:\n",
    "        noise, model, dataset, seed = details_from_key(k)\n",
    "        SWEEP_FLAGS = f\"{_SWEEP_FEATS8_FLAGS} --noise_std={noise}\"\n",
    "        SWEEP_PREFIX = f\"{_SWEEP_PREFIX}{noise}\"\n",
    "        print(f'./run_sweep.sh {dataset} {seed} {seed} {model} {partition} \"{SWEEP_FLAGS}\" {SWEEP_PREFIX}')\n",
    "        !./run_sweep.sh $dataset $seed $seed $model $partition \"$SWEEP_FLAGS\" $SWEEP_PREFIX\n",
    "        launched.add(k)\n",
    "\n",
    "def resubmit_pending_agents(launched, sweep_ids, n_agents=1, partition=\"cpu\"):\n",
    "    pending = !sacct --format=\"JobID,JobName%50,Partition,State\" | grep PENDING\n",
    "    print('\\n'.join(pending))\n",
    "    _pending_agents = set()\n",
    "    _pending_sweep_job_ids = []\n",
    "    for pen in pending:\n",
    "        pen_split = pen.split()\n",
    "        SWEEP_PREFIX, model, dataset, seed, _ = pen_split[1].split('_')\n",
    "        assert _SWEEP_PREFIX in SWEEP_PREFIX\n",
    "        noise = SWEEP_PREFIX[-1]\n",
    "        seed_split = seed.split('-')  # remove \"agentX\"\n",
    "        seed = seed_split[0][-1]\n",
    "        if len(seed_split) > 1:\n",
    "            # Sweep agent job is pending\n",
    "            _pending_agents.add(make_key(noise, model, dataset, seed))\n",
    "            _pending_sweep_job_ids.append(pen_split[0])\n",
    "    print(f\"PENDING agents: {_pending_agents}\\n\")\n",
    "    if len(_pending_agents) > 0:\n",
    "        # Cancel pending\n",
    "        _pending_sweep_job_ids = ' '.join(_pending_sweep_job_ids)\n",
    "        print(f\"!scancel {_pending_sweep_job_ids}\")\n",
    "        !scancel $_pending_sweep_job_ids\n",
    "        # Relaunch\n",
    "        add_agents_by_list(keys_to_add=_pending_agents, launched=launched,\n",
    "                           sweep_ids=sweep_ids, n_agents=n_agents, partition=partition)\n",
    "\n",
    "def resubmit_pending_sweeps(launched, partition=\"cpu\"):\n",
    "    pending = !sacct --format=\"JobID,JobName%50,Partition,State\" | grep PENDING\n",
    "    print('\\n'.join(pending))\n",
    "    _pending_agents = set()\n",
    "    _pending_sweep_job_ids = []\n",
    "    for pen in pending:\n",
    "        pen_split = pen.split()\n",
    "        SWEEP_PREFIX, model, dataset, seed, _ = pen_split[1].split('_')\n",
    "        assert _SWEEP_PREFIX in SWEEP_PREFIX\n",
    "        noise = SWEEP_PREFIX[-1]\n",
    "        seed_split = seed.split('-')  # remove \"agentX\"\n",
    "        seed = seed_split[0][-1]\n",
    "        if len(seed_split) == 1:\n",
    "            # Sweep init job is pending\n",
    "            _pending_agents.add(make_key(noise, model, dataset, seed))\n",
    "            _pending_sweep_job_ids.append(pen_split[0])\n",
    "    print(f\"PENDING agents: {_pending_agents}\\n\")\n",
    "    if len(_pending_agents) > 0:\n",
    "        # Cancel pending\n",
    "        _pending_sweep_job_ids = ' '.join(_pending_sweep_job_ids)\n",
    "        print(f\"!scancel {_pending_sweep_job_ids}\")\n",
    "        !scancel $_pending_sweep_job_ids\n",
    "        # Relaunch\n",
    "        add_sweeps_by_list(keys_to_add=_pending_agents, launched=launched, partition=partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e69efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "launched = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecf917",
   "metadata": {},
   "outputs": [],
   "source": [
    "resubmit_pending_sweeps(launched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120e03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "add_agents(max_run_count=60, launched=launched, finished_runs=finished_runs, sweep_ids=sweep_ids, n_agents=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resubmit_pending_agents(launched=launched, sweep_ids=sweep_ids, n_agents=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59090fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9a40ba4",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62364eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_dfs(best_runs):\n",
    "    res_map = {\n",
    "        'train_time': 'z_run_time',\n",
    "        'inf_time_hac': 'z_inf_time_hac',\n",
    "        'inf_time_cc': 'z_inf_time_cc',\n",
    "        'inf_time_cc-nosdp': 'z_inf_time_cc-nosdp',\n",
    "        'b3_f1_hac': 'best_test_b3_f1_hac',\n",
    "        'b3_f1_cc': 'best_test_b3_f1_cc',\n",
    "        'b3_f1_cc-fixed': 'best_test_b3_f1_cc-fixed',\n",
    "        'b3_f1_cc-nosdp': 'best_test_b3_f1_cc-nosdp',\n",
    "        'b3_f1_cc-nosdp-fixed': 'best_test_b3_f1_cc-nosdp-fixed',\n",
    "        'vmeasure_hac': 'best_test_vmeasure_hac',\n",
    "        'vmeasure_cc': 'best_test_vmeasure_cc',\n",
    "        'vmeasure_cc-fixed': 'best_test_vmeasure_cc-fixed',\n",
    "        'vmeasure_cc-nosdp': 'best_test_vmeasure_cc-nosdp',\n",
    "        'vmeasure_cc-nosdp-fixed': 'best_test_vmeasure_cc-nosdp-fixed'\n",
    "    }\n",
    "    final = {}\n",
    "    for run_id, run in best_runs.items():\n",
    "        _key = run_id[:-2]  # Remove the seed\n",
    "        if _key not in final:\n",
    "            final[_key] = defaultdict(list)\n",
    "        res = dict(run.summary)\n",
    "        for out_key, in_key in res_map.items():\n",
    "            final[_key][out_key].append(float(res[in_key]))\n",
    "    means, stds, comb = {}, {}, {}\n",
    "    for k in final:\n",
    "        if k is not means:\n",
    "            means[k] = {}\n",
    "            stds[k] = {}\n",
    "            comb[k] = {}\n",
    "        for _k in final[k]:\n",
    "            means[k][_k] = round(np.mean(final[k][_k])*(1 if 'time' in _k else 100), 2)\n",
    "            stds[k][_k] = round(np.std(final[k][_k])*(1 if 'time' in _k else 100), 2)\n",
    "            comb[k][_k] = f\"{means[k][_k]}±{stds[k][_k]}\"\n",
    "    return means, stds, comb\n",
    "\n",
    "\n",
    "\n",
    "def get_df_by_dataset(res, dataset, noise, to_latex=False):\n",
    "    new_res = {}\n",
    "    for _r in res:\n",
    "        if dataset in _r and f'noise{noise}' in _r:\n",
    "            _new_r = _r.replace(f\"{dataset}_\", '').replace(f\"_{dataset}\", '')\n",
    "            _new_r = _new_r.replace(f\"_noise{noise}\", '').replace(f\"noise{noise}_\", '')\n",
    "            new_res[_new_r] = res[_r]\n",
    "    if to_latex:\n",
    "        print(pd.DataFrame(new_res).T.style.to_latex())\n",
    "    outdf = pd.DataFrame(new_res).T.sort_index()\n",
    "    def highlight_max(s):\n",
    "        if s.dtype == object:\n",
    "            is_max = [False for _ in range(s.shape[0])]\n",
    "            if '±' in s[0]:\n",
    "                nums = np.array(list(map(lambda x: float(x.split('±')[0]), s)))\n",
    "                is_max = nums == nums.max()\n",
    "        else:\n",
    "            is_max = s == s.max()\n",
    "        return ['color: green' if cell else '' for cell in is_max]\n",
    "    \n",
    "    if outdf[outdf.keys()[0]].dtype == object:\n",
    "        return outdf.style.apply(highlight_max), outdf\n",
    "    return outdf.style.format('{:.2f}').apply(highlight_max), outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da91a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "means, stds, comb = get_result_dfs(best_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf, _outdf = get_df_by_dataset(res=comb, dataset='pubmed', noise=1)\n",
    "outdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
