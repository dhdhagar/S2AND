{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44be237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import higra as hg\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9758c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device=cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(f'Using device={device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5f4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hac_gpu_single(X, _MAX_DIST=10, verbose=False, device=device):\n",
    "    # Initialization\n",
    "    D = X.size(1)\n",
    "    # Take the upper triangular and mask the other values with a large number\n",
    "    Y = _MAX_DIST*torch.ones(D,D, device=device).tril() + X.triu(1)\n",
    "    parents = torch.arange(D+(D-1), device=device)\n",
    "    parent_to_idx = torch.arange(D+(D-1), device=device)\n",
    "    idx_to_parent = torch.arange(D, device=device)\n",
    "    values, indices = torch.min(Y, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Initialization:')\n",
    "        print('Y:', Y)\n",
    "        print('\\tparents:', parents)\n",
    "        print('\\tparent_to_idx:', parent_to_idx)\n",
    "        print('\\tidx_to_parent:', idx_to_parent)\n",
    "        print('\\tminima (values):', values)\n",
    "        print('\\tminima (indices):', indices)\n",
    "        print()\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    max_node = D-1\n",
    "    if verbose:\n",
    "        print('Starting algorithm:')\n",
    "    for i in range(D-1):\n",
    "        max_node += 1\n",
    "        min_minima_idx = torch.argmin(values).item()\n",
    "\n",
    "        # Merge the index of the minimum value of minimums across rows with the index of the minimum value in its row\n",
    "        merge_idx_1 = min_minima_idx\n",
    "        merge_idx_2 = indices[merge_idx_1].item()\n",
    "\n",
    "        # Find highest-altitude clusters corresponding to the merge indices\n",
    "        parent_1 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_1]]]\n",
    "        parent_2 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_2]]]\n",
    "\n",
    "        if verbose:\n",
    "            print(f'    #{i} Merging:',(merge_idx_1, merge_idx_2),'i.e.', (parent_1.item(), parent_2.item()), \n",
    "                  '=>', max_node)\n",
    "\n",
    "        # Add parent for the clusters being merged\n",
    "        parents[parent_1] = max_node\n",
    "        parents[parent_2] = max_node\n",
    "\n",
    "        # Update mappings\n",
    "        idx_to_parent[merge_idx_1] = max_node\n",
    "        parent_to_idx[max_node] = merge_idx_1\n",
    "\n",
    "        # Update the matrix with merged values for cluster similarities\n",
    "        max_dist_mask = Y == _MAX_DIST\n",
    "        new_merge_idx_1_values = torch.min(torch.min(Y[merge_idx_1, :], Y[:, merge_idx_2]), \n",
    "                                           torch.min(Y[:, merge_idx_1], Y[merge_idx_2, :]))\n",
    "        Y[:, merge_idx_1] = new_merge_idx_1_values\n",
    "        Y[merge_idx_1, :] = new_merge_idx_1_values\n",
    "        Y[max_dist_mask] = _MAX_DIST\n",
    "        Y[:, merge_idx_2] = _MAX_DIST\n",
    "        Y[merge_idx_2, :] = _MAX_DIST\n",
    "\n",
    "        # Update nearest neighbour trackers\n",
    "        values[merge_idx_2] = _MAX_DIST\n",
    "        indices[indices == merge_idx_2] = merge_idx_1\n",
    "        new_min_idx = torch.argmin(Y[merge_idx_1, merge_idx_1+1:]) + (merge_idx_1 + 1)\n",
    "        values[min_minima_idx] = Y[merge_idx_1, new_min_idx]\n",
    "        indices[min_minima_idx] = new_min_idx\n",
    "\n",
    "        if verbose:\n",
    "            print('Y:', Y)\n",
    "            print('\\tminima (values):', values)\n",
    "            print('\\tminima (indices):', indices)\n",
    "            print('\\tparents:', parents)\n",
    "            print('\\tparent_to_idx:', parent_to_idx)\n",
    "            print('\\tidx_to_parent:', idx_to_parent)\n",
    "            print()\n",
    "    \n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca425c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "01477888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents_from_higra(Z, linkage):\n",
    "    _g = hg.UndirectedGraph(D)\n",
    "    _g.add_edges(torch.triu_indices(D,D,1).numpy()[0], torch.triu_indices(D,D,1).numpy()[1])\n",
    "    _data = Z.cpu()[torch.triu_indices(D,D,1)[0],torch.triu_indices(D,D,1)[1]].numpy()\n",
    "    hg_func = {\n",
    "        'single': hg.binary_partition_tree_single_linkage,\n",
    "        'average': hg.binary_partition_tree_average_linkage,\n",
    "    }\n",
    "    _hg_hac = hg_func[linkage](_g, _data)\n",
    "    return _hg_hac[0].parents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f15370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "81e4007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.5314, 0.7572,  ..., 0.5863, 0.6819, 0.4844],\n",
      "        [0.5314, 1.0000, 0.4148,  ..., 0.2384, 0.0924, 0.4560],\n",
      "        [0.7572, 0.4148, 1.0000,  ..., 0.2976, 0.1460, 0.8615],\n",
      "        ...,\n",
      "        [0.5863, 0.2384, 0.2976,  ..., 1.0000, 0.5498, 0.2335],\n",
      "        [0.6819, 0.0924, 0.1460,  ..., 0.5498, 1.0000, 0.8720],\n",
      "        [0.4844, 0.4560, 0.8615,  ..., 0.2335, 0.8720, 1.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(15)\n",
    "\n",
    "# Construct a random, symmetric matrix of values between 0 and 1\n",
    "D = 4000\n",
    "X = torch.rand((D,D), device=device)\n",
    "X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ebc30974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test suite to verify accuracy of solution\n",
    "\n",
    "for D in range(10,101,10):\n",
    "    # D = 5\n",
    "    X = torch.rand((D,D), device=device)\n",
    "    X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "\n",
    "    # Get HAC parents from GPU code\n",
    "    _parents = hac_gpu_single(X, verbose=False)\n",
    "#     print('GPU HAC parents:', _parents.tolist())\n",
    "\n",
    "    # Get HAC parents from Higra\n",
    "    _hg_parents = get_parents_from_higra(X, linkage='single')\n",
    "#     print('Higra HAC parents:', list(_hg_parents))\n",
    "\n",
    "    assert np.array_equal(_parents.tolist(), list(_hg_parents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7233c09",
   "metadata": {},
   "source": [
    "#### Measure performance difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours: Single-linkage\n",
    "\n",
    "# %%timeit\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "_parents = hac_gpu(X, verbose=False)\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93caa64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higra: Single-linkage\n",
    "\n",
    "# %%timeit\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "_hg_parents = get_parents_from_higra(X, linkage='single')\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf0fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aef7d598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff7990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "157aacfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd3b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5d1bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STABLE; v1\n",
    "\n",
    "def hac_gpu_avg(X, _MAX_DIST=10, verbose=False, device=device):\n",
    "    # Initialization\n",
    "    D = X.size(1)\n",
    "    # Take the upper triangular and mask the other values with a large number\n",
    "    Y = _MAX_DIST*torch.ones(D,D, device='cuda').tril() + X.triu(1)\n",
    "    parents = torch.arange(D+(D-1), device=device)\n",
    "    parent_to_idx = torch.arange(D+(D-1), device=device)\n",
    "    idx_to_parent = torch.arange(D, device=device)\n",
    "    cluster_sizes = torch.ones(D+(D-1), device=device)\n",
    "    values, indices = torch.min(Y, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Initialization:')\n",
    "        print('Y:', Y)\n",
    "        print('\\tparents:', parents)\n",
    "        print('\\tparent_to_idx:', parent_to_idx)\n",
    "        print('\\tidx_to_parent:', idx_to_parent)\n",
    "        print('\\tminima (values):', values)\n",
    "        print('\\tminima (indices):', indices)\n",
    "        print('\\tcluster_sizes:', cluster_sizes)\n",
    "        print()\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    max_node = D-1\n",
    "    if verbose:\n",
    "        print('Starting algorithm:')\n",
    "    for i in range(D-1):\n",
    "        max_node += 1\n",
    "        min_minima_idx = torch.argmin(values).item()\n",
    "\n",
    "        # Merge the index of the minimum value of minimums across rows with the index of the minimum value in its row\n",
    "        merge_idx_1 = min_minima_idx\n",
    "        merge_idx_2 = indices[merge_idx_1].item()\n",
    "\n",
    "        # Find highest-altitude clusters corresponding to the merge indices\n",
    "        parent_1 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_1]]].item()\n",
    "        parent_2 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_2]]].item()\n",
    "\n",
    "        if verbose:\n",
    "            print(f'    #{i} Merging:',(merge_idx_1, merge_idx_2),'i.e.', (parent_1, parent_2), '=>', max_node)\n",
    "\n",
    "        # Add parent for the clusters being merged\n",
    "        parents[parent_1] = max_node\n",
    "        parents[parent_2] = max_node\n",
    "\n",
    "        # Update mappings\n",
    "        idx_to_parent[merge_idx_1] = max_node\n",
    "        parent_to_idx[max_node] = merge_idx_1\n",
    "\n",
    "        # Update the matrix with merged values for cluster similarities\n",
    "        max_dist_mask = Y == _MAX_DIST\n",
    "        new_cluster_size = cluster_sizes[parent_1] + cluster_sizes[parent_2]\n",
    "        cluster_sizes[max_node] = new_cluster_size\n",
    "        new_merge_idx_1_values = (torch.min(Y[merge_idx_1, :], Y[:, merge_idx_1]) * cluster_sizes[parent_1] + \\\n",
    "                                  torch.min(Y[:, merge_idx_2], Y[merge_idx_2, :]) * cluster_sizes[parent_2]) / \\\n",
    "                                    new_cluster_size\n",
    "        Y[:, merge_idx_1] = new_merge_idx_1_values\n",
    "        Y[merge_idx_1, :] = new_merge_idx_1_values\n",
    "        Y[max_dist_mask] = _MAX_DIST\n",
    "        Y[:, merge_idx_2] = _MAX_DIST\n",
    "        Y[merge_idx_2, :] = _MAX_DIST\n",
    "\n",
    "        # Update nearest neighbour trackers\n",
    "        values[merge_idx_2] = _MAX_DIST\n",
    "        \n",
    "        max_dist_mask = values == _MAX_DIST\n",
    "        values, indices = torch.min(Y, dim=1)\n",
    "        values[max_dist_mask] = _MAX_DIST\n",
    "\n",
    "        if verbose:\n",
    "            print('Y:', Y)\n",
    "            print('\\tminima (values):', values)\n",
    "            print('\\tminima (indices):', indices)\n",
    "            print('\\tparents:', parents)\n",
    "            print('\\tparent_to_idx:', parent_to_idx)\n",
    "            print('\\tidx_to_parent:', idx_to_parent)\n",
    "            print('\\tcluster_sizes:', cluster_sizes)\n",
    "            print()\n",
    "    \n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3ada4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8396, 0.3640,  ..., 0.9660, 0.3892, 0.4684],\n",
      "        [0.8396, 1.0000, 0.1113,  ..., 0.4993, 0.6405, 0.4826],\n",
      "        [0.3640, 0.1113, 1.0000,  ..., 0.8844, 0.7767, 0.3587],\n",
      "        ...,\n",
      "        [0.9660, 0.4993, 0.8844,  ..., 1.0000, 0.7781, 0.7506],\n",
      "        [0.3892, 0.6405, 0.7767,  ..., 0.7781, 1.0000, 0.9799],\n",
      "        [0.4684, 0.4826, 0.3587,  ..., 0.7506, 0.9799, 1.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Construct a random, symmetric matrix of values between 0 and 1\n",
    "D = 4000\n",
    "X = torch.rand((D,D), device=device)\n",
    "X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2dc3036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU HAC parents: [6, 8, 6, 7, 8, 7, 9, 9, 10, 10, 10]\n",
      "Higra HAC parents: [6, 8, 6, 7, 8, 7, 9, 9, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "# Get HAC parents from GPU code\n",
    "_parents = hac_gpu_avg(X, verbose=False, device='cpu')\n",
    "print('GPU HAC parents:', _parents.tolist())\n",
    "\n",
    "# Get HAC parents from Higra\n",
    "_hg_parents = get_parents_from_higra(X, linkage='average')\n",
    "print('Higra HAC parents:', list(_hg_parents))\n",
    "\n",
    "assert np.array_equal(_parents.tolist(),list(_hg_parents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ea817fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test suite\n",
    "\n",
    "for D in range(10,101,10):\n",
    "    X = torch.rand((D,D), device=device)\n",
    "    X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "\n",
    "    # Get HAC parents from GPU code\n",
    "    _parents = hac_gpu_avg(X, verbose=False)\n",
    "#     print('GPU HAC parents:', _parents.tolist())\n",
    "\n",
    "    # Get HAC parents from Higra\n",
    "    _hg_parents = get_parents_from_higra(X, linkage='average')\n",
    "#     print('Higra HAC parents:', list(_hg_parents))\n",
    "\n",
    "    assert np.array_equal(_parents.tolist(), list(_hg_parents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0db1b",
   "metadata": {},
   "source": [
    "#### Measure performance difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6396d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.46e+00s\n"
     ]
    }
   ],
   "source": [
    "# Ours: Average-linkage\n",
    "\n",
    "# %%timeit\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "_parents = hac_gpu_avg(X, verbose=False, device='cpu')\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ce257432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.86e+01s\n"
     ]
    }
   ],
   "source": [
    "# Higra: Average-linkage\n",
    "\n",
    "# %%timeit\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "_hg_parents = get_parents_from_higra(X, linkage='average')\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323894d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ec24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3dfa27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1be18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "a39309e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct hac-cut rounding\n",
    "\n",
    "def avg_hac_cut(X, weights, _MAX_DIST=10, verbose=False, device='cpu', \n",
    "                max_similarity=1, use_similarities=False):\n",
    "    # Initialization\n",
    "    D = X.size(1)\n",
    "    parents = torch.arange(D+(D-1))\n",
    "    parent_to_idx = torch.arange(D+(D-1))\n",
    "    idx_to_parent = torch.arange(D)\n",
    "    cluster_sizes = torch.ones(D+(D-1))\n",
    "    \n",
    "    energy = torch.zeros(D+(D-1), device=device)\n",
    "    clustering = torch.zeros((D+(D-1), D))\n",
    "    clustering[torch.arange(D),torch.arange(D)] = torch.arange(1,D+1, dtype=clustering.dtype)\n",
    "    round_matrix = torch.eye(D, device=device)\n",
    "    \n",
    "    # Take the upper triangular and mask the other values with a large number\n",
    "    Y = _MAX_DIST * torch.ones(D, D, device=device).tril() + (max_similarity-X if use_similarities else X).triu(1)\n",
    "    # Compute the dissimilarity minima per row\n",
    "    values, indices = torch.min(Y, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Initialization:')\n",
    "        print('Y:', Y)\n",
    "        print('\\tparents:', parents)\n",
    "        print('\\tparent_to_idx:', parent_to_idx)\n",
    "        print('\\tidx_to_parent:', idx_to_parent)\n",
    "        print('\\tminima (values):', values)\n",
    "        print('\\tminima (indices):', indices)\n",
    "        print('\\tcluster_sizes:', cluster_sizes)\n",
    "        print()\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    max_node = D-1\n",
    "    if verbose:\n",
    "        print('Starting algorithm:')\n",
    "    for i in range(D-1):\n",
    "        max_node += 1\n",
    "        min_minima_idx = torch.argmin(values).item()\n",
    "\n",
    "        # Merge the index of the minimum value of minimums across rows with the index of the minimum value in its row\n",
    "        merge_idx_1 = min_minima_idx\n",
    "        merge_idx_2 = indices[merge_idx_1].item()\n",
    "\n",
    "        # Find highest-altitude clusters corresponding to the merge indices\n",
    "        parent_1 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_1]]].item()\n",
    "        parent_2 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_2]]].item()\n",
    "\n",
    "        if verbose:\n",
    "            print(f'    #{i} Merging:',(merge_idx_1, merge_idx_2),'i.e.', (parent_1, parent_2), '=>', max_node)\n",
    "\n",
    "        # Add parent for the clusters being merged\n",
    "        parents[parent_1] = max_node\n",
    "        parents[parent_2] = max_node\n",
    "\n",
    "        # Update mappings\n",
    "        idx_to_parent[merge_idx_1] = max_node\n",
    "        parent_to_idx[max_node] = merge_idx_1\n",
    "\n",
    "        # Update the matrix with merged values for cluster similarities\n",
    "        max_dist_mask = Y == _MAX_DIST\n",
    "        new_cluster_size = cluster_sizes[parent_1] + cluster_sizes[parent_2]\n",
    "        cluster_sizes[max_node] = new_cluster_size\n",
    "        new_merge_idx_1_values = (torch.min(Y[merge_idx_1, :], Y[:, merge_idx_1]) * cluster_sizes[parent_1] + \\\n",
    "                                  torch.min(Y[:, merge_idx_2], Y[merge_idx_2, :]) * cluster_sizes[parent_2]) / \\\n",
    "                                    new_cluster_size\n",
    "        Y[:, merge_idx_1] = new_merge_idx_1_values\n",
    "        Y[merge_idx_1, :] = new_merge_idx_1_values\n",
    "        Y[max_dist_mask] = _MAX_DIST\n",
    "        Y[:, merge_idx_2] = _MAX_DIST\n",
    "        Y[merge_idx_2, :] = _MAX_DIST\n",
    "\n",
    "        # Update nearest neighbour trackers\n",
    "        values[merge_idx_2] = _MAX_DIST\n",
    "        \n",
    "        max_dist_mask = values == _MAX_DIST\n",
    "        values, indices = torch.min(Y, dim=1)\n",
    "        values[max_dist_mask] = _MAX_DIST\n",
    "        \n",
    "        # Energy calculations\n",
    "        clustering[max_node] = clustering[parent_1] + clustering[parent_2]\n",
    "        leaf_indices = torch.where(clustering[max_node])[0]\n",
    "        leaf_edges = torch.meshgrid(leaf_indices, leaf_indices)\n",
    "        energy[max_node] = energy[parent_1] + energy[parent_2]\n",
    "        merge_energy = torch.sum(weights[leaf_edges])\n",
    "        if merge_energy >= energy[max_node]:\n",
    "            energy[max_node] = merge_energy\n",
    "            clustering[max_node][clustering[max_node] > 0] = max_node\n",
    "            round_matrix[leaf_edges] = 1\n",
    "        \n",
    "        if verbose:\n",
    "            print('Y:', Y)\n",
    "            print('\\tminima (values):', values)\n",
    "            print('\\tminima (indices):', indices)\n",
    "            print('\\tparents:', parents)\n",
    "            print('\\tparent_to_idx:', parent_to_idx)\n",
    "            print('\\tidx_to_parent:', idx_to_parent)\n",
    "            print('\\tcluster_sizes:', cluster_sizes)\n",
    "            print('\\tclustering (current):', clustering[max_node])\n",
    "            print('round_matrix:')\n",
    "            print(round_matrix)\n",
    "            print()\n",
    "    \n",
    "    return round_matrix, clustering[-1], parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843b37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "5bacecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a random, symmetric matrix of values between 0 and 1\n",
    "D = 1000\n",
    "X = torch.rand((D,D), device=device)\n",
    "X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "# print(X)\n",
    "\n",
    "W = torch.rand((D,D), device=device) * 2 - 1\n",
    "W = W.triu(1) + torch.zeros((D,D), device=device)\n",
    "# print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "0d311f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78e+00s\n"
     ]
    }
   ],
   "source": [
    "# Higra: Average-linkage\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "higra_parents = get_parents_from_higra(1-X, linkage='average')\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "32db10a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.80e-01s\n"
     ]
    }
   ],
   "source": [
    "# Ours: Average-linkage\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "result = avg_hac_cut(X, W, verbose=False, device='cuda', use_similarities=True)\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "d6ee72c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(list(higra_parents), result[2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "98f9effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = torch.triu_indices(D,D,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "2719eae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.unique(X[r,c])) == len(X[r,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "d6bd723f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494480"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.unique(X[r,c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "bd835eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499500"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[r,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "463a49c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(higra_parents != result[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9aaa784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e728585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "triu_indices = torch.triu_indices(D,D,1)\n",
    "equals, dups = [], []\n",
    "for i in tqdm(range(1000)):\n",
    "    D = 1000\n",
    "    X = torch.rand((D,D), device=device)\n",
    "    X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "    W = torch.rand((D,D), device=device) * 2 - 1\n",
    "    W = W.triu(1) + torch.zeros((D,D), device=device)\n",
    "    \n",
    "    result = avg_hac_cut(X, W, verbose=False, device='cuda', use_similarities=True)\n",
    "    higra_parents = get_parents_from_higra(1-X, linkage='average')\n",
    "    \n",
    "    equal_output = np.array_equal(list(higra_parents), result[2].tolist())\n",
    "    has_dups = len(torch.unique((1-X)[triu_indices[0], triu_indices[1]])) < len(triu_indices[0])\n",
    "    equals.append(equal_output)\n",
    "    dups.append(has_dups)\n",
    "    assert equal_output or has_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a79771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fca9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
