{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44be237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import higra as hg\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9758c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device=cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(f'Using device={device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5f4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hac_gpu_single(X, _MAX_DIST=10, verbose=False, device=device):\n",
    "    # Initialization\n",
    "    D = X.size(1)\n",
    "    # Take the upper triangular and mask the other values with a large number\n",
    "    Y = _MAX_DIST*torch.ones(D,D, device=device).tril() + X.triu(1)\n",
    "    parents = torch.arange(D+(D-1), device=device)\n",
    "    parent_to_idx = torch.arange(D+(D-1), device=device)\n",
    "    idx_to_parent = torch.arange(D, device=device)\n",
    "    values, indices = torch.min(Y, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Initialization:')\n",
    "        print('Y:', Y)\n",
    "        print('\\tparents:', parents)\n",
    "        print('\\tparent_to_idx:', parent_to_idx)\n",
    "        print('\\tidx_to_parent:', idx_to_parent)\n",
    "        print('\\tminima (values):', values)\n",
    "        print('\\tminima (indices):', indices)\n",
    "        print()\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    max_node = D-1\n",
    "    if verbose:\n",
    "        print('Starting algorithm:')\n",
    "    for i in range(D-1):\n",
    "        max_node += 1\n",
    "        min_minima_idx = torch.argmin(values).item()\n",
    "\n",
    "        # Merge the index of the minimum value of minimums across rows with the index of the minimum value in its row\n",
    "        merge_idx_1 = min_minima_idx\n",
    "        merge_idx_2 = indices[merge_idx_1].item()\n",
    "\n",
    "        # Find highest-altitude clusters corresponding to the merge indices\n",
    "        parent_1 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_1]]]\n",
    "        parent_2 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_2]]]\n",
    "\n",
    "        if verbose:\n",
    "            print(f'    #{i} Merging:',(merge_idx_1, merge_idx_2),'i.e.', (parent_1.item(), parent_2.item()), \n",
    "                  '=>', max_node)\n",
    "\n",
    "        # Add parent for the clusters being merged\n",
    "        parents[parent_1] = max_node\n",
    "        parents[parent_2] = max_node\n",
    "\n",
    "        # Update mappings\n",
    "        idx_to_parent[merge_idx_1] = max_node\n",
    "        parent_to_idx[max_node] = merge_idx_1\n",
    "\n",
    "        # Update the matrix with merged values for cluster similarities\n",
    "        max_dist_mask = Y == _MAX_DIST\n",
    "        new_merge_idx_1_values = torch.min(torch.min(Y[merge_idx_1, :], Y[:, merge_idx_2]), \n",
    "                                           torch.min(Y[:, merge_idx_1], Y[merge_idx_2, :]))\n",
    "        Y[:, merge_idx_1] = new_merge_idx_1_values\n",
    "        Y[merge_idx_1, :] = new_merge_idx_1_values\n",
    "        Y[max_dist_mask] = _MAX_DIST\n",
    "        Y[:, merge_idx_2] = _MAX_DIST\n",
    "        Y[merge_idx_2, :] = _MAX_DIST\n",
    "\n",
    "        # Update nearest neighbour trackers\n",
    "        values[merge_idx_2] = _MAX_DIST\n",
    "        indices[indices == merge_idx_2] = merge_idx_1\n",
    "        new_min_idx = torch.argmin(Y[merge_idx_1, merge_idx_1+1:]) + (merge_idx_1 + 1)\n",
    "        values[min_minima_idx] = Y[merge_idx_1, new_min_idx]\n",
    "        indices[min_minima_idx] = new_min_idx\n",
    "\n",
    "        if verbose:\n",
    "            print('Y:', Y)\n",
    "            print('\\tminima (values):', values)\n",
    "            print('\\tminima (indices):', indices)\n",
    "            print('\\tparents:', parents)\n",
    "            print('\\tparent_to_idx:', parent_to_idx)\n",
    "            print('\\tidx_to_parent:', idx_to_parent)\n",
    "            print()\n",
    "    \n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca425c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01477888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents_from_higra(X, linkage):\n",
    "    _g = hg.UndirectedGraph(D)\n",
    "    _g.add_edges(torch.triu_indices(D,D,1).numpy()[0], torch.triu_indices(D,D,1).numpy()[1])\n",
    "    _data = X.cpu()[torch.triu_indices(D,D,1)[0],torch.triu_indices(D,D,1)[1]].numpy()\n",
    "    hg_func = {\n",
    "        'single': hg.binary_partition_tree_single_linkage,\n",
    "        'average': hg.binary_partition_tree_average_linkage,\n",
    "    }\n",
    "    _hg_hac = hg_func[linkage](_g, _data)\n",
    "    return _hg_hac[0].parents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f15370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "81e4007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.5314, 0.7572,  ..., 0.5863, 0.6819, 0.4844],\n",
      "        [0.5314, 1.0000, 0.4148,  ..., 0.2384, 0.0924, 0.4560],\n",
      "        [0.7572, 0.4148, 1.0000,  ..., 0.2976, 0.1460, 0.8615],\n",
      "        ...,\n",
      "        [0.5863, 0.2384, 0.2976,  ..., 1.0000, 0.5498, 0.2335],\n",
      "        [0.6819, 0.0924, 0.1460,  ..., 0.5498, 1.0000, 0.8720],\n",
      "        [0.4844, 0.4560, 0.8615,  ..., 0.2335, 0.8720, 1.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(15)\n",
    "\n",
    "# Construct a random, symmetric matrix of values between 0 and 1\n",
    "D = 4000\n",
    "X = torch.rand((D,D), device=device)\n",
    "X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ebc30974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test suite to verify accuracy of solution\n",
    "\n",
    "for D in range(10,101,10):\n",
    "    # D = 5\n",
    "    X = torch.rand((D,D), device=device)\n",
    "    X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "\n",
    "    # Get HAC parents from GPU code\n",
    "    _parents = hac_gpu_single(X, verbose=False)\n",
    "#     print('GPU HAC parents:', _parents.tolist())\n",
    "\n",
    "    # Get HAC parents from Higra\n",
    "    _hg_parents = get_parents_from_higra(X, linkage='single')\n",
    "#     print('Higra HAC parents:', list(_hg_parents))\n",
    "\n",
    "    assert np.array_equal(_parents.tolist(), list(_hg_parents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6a40c",
   "metadata": {},
   "source": [
    "#### Measure performance difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ours: Single-linkage\n",
    "\n",
    "# %%timeit\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "_parents = hac_gpu(X, verbose=False)\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93caa64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higra: Single-linkage\n",
    "\n",
    "# %%timeit\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "_hg_parents = get_parents_from_higra(X, linkage='single')\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf0fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aef7d598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff7990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "157aacfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd3b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5d1bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STABLE; v1\n",
    "\n",
    "def hac_gpu_avg(X, _MAX_DIST=10, verbose=False, device=device):\n",
    "    # Initialization\n",
    "    D = X.size(1)\n",
    "    # Take the upper triangular and mask the other values with a large number\n",
    "    Y = _MAX_DIST*torch.ones(D,D, device='cuda').tril() + X.triu(1)\n",
    "    parents = torch.arange(D+(D-1), device=device)\n",
    "    parent_to_idx = torch.arange(D+(D-1), device=device)\n",
    "    idx_to_parent = torch.arange(D, device=device)\n",
    "    cluster_sizes = torch.ones(D+(D-1), device=device)\n",
    "    values, indices = torch.min(Y, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Initialization:')\n",
    "        print('Y:', Y)\n",
    "        print('\\tparents:', parents)\n",
    "        print('\\tparent_to_idx:', parent_to_idx)\n",
    "        print('\\tidx_to_parent:', idx_to_parent)\n",
    "        print('\\tminima (values):', values)\n",
    "        print('\\tminima (indices):', indices)\n",
    "        print('\\tcluster_sizes:', cluster_sizes)\n",
    "        print()\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    max_node = D-1\n",
    "    if verbose:\n",
    "        print('Starting algorithm:')\n",
    "    for i in range(D-1):\n",
    "        max_node += 1\n",
    "        min_minima_idx = torch.argmin(values).item()\n",
    "\n",
    "        # Merge the index of the minimum value of minimums across rows with the index of the minimum value in its row\n",
    "        merge_idx_1 = min_minima_idx\n",
    "        merge_idx_2 = indices[merge_idx_1].item()\n",
    "\n",
    "        # Find highest-altitude clusters corresponding to the merge indices\n",
    "        parent_1 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_1]]].item()\n",
    "        parent_2 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_2]]].item()\n",
    "\n",
    "        if verbose:\n",
    "            print(f'    #{i} Merging:',(merge_idx_1, merge_idx_2),'i.e.', (parent_1, parent_2), '=>', max_node)\n",
    "\n",
    "        # Add parent for the clusters being merged\n",
    "        parents[parent_1] = max_node\n",
    "        parents[parent_2] = max_node\n",
    "\n",
    "        # Update mappings\n",
    "        idx_to_parent[merge_idx_1] = max_node\n",
    "        parent_to_idx[max_node] = merge_idx_1\n",
    "\n",
    "        # Update the matrix with merged values for cluster similarities\n",
    "        max_dist_mask = Y == _MAX_DIST\n",
    "        new_cluster_size = cluster_sizes[parent_1] + cluster_sizes[parent_2]\n",
    "        cluster_sizes[max_node] = new_cluster_size\n",
    "        new_merge_idx_1_values = (torch.min(Y[merge_idx_1, :], Y[:, merge_idx_1]) * cluster_sizes[parent_1] + \\\n",
    "                                  torch.min(Y[:, merge_idx_2], Y[merge_idx_2, :]) * cluster_sizes[parent_2]) / \\\n",
    "                                    new_cluster_size\n",
    "        Y[:, merge_idx_1] = new_merge_idx_1_values\n",
    "        Y[merge_idx_1, :] = new_merge_idx_1_values\n",
    "        Y[max_dist_mask] = _MAX_DIST\n",
    "        Y[:, merge_idx_2] = _MAX_DIST\n",
    "        Y[merge_idx_2, :] = _MAX_DIST\n",
    "\n",
    "        # Update nearest neighbour trackers\n",
    "        values[merge_idx_2] = _MAX_DIST\n",
    "        \n",
    "        max_dist_mask = values == _MAX_DIST\n",
    "        values, indices = torch.min(Y, dim=1)\n",
    "        values[max_dist_mask] = _MAX_DIST\n",
    "\n",
    "        if verbose:\n",
    "            print('Y:', Y)\n",
    "            print('\\tminima (values):', values)\n",
    "            print('\\tminima (indices):', indices)\n",
    "            print('\\tparents:', parents)\n",
    "            print('\\tparent_to_idx:', parent_to_idx)\n",
    "            print('\\tidx_to_parent:', idx_to_parent)\n",
    "            print('\\tcluster_sizes:', cluster_sizes)\n",
    "            print()\n",
    "    \n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3ada4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8396, 0.3640,  ..., 0.9660, 0.3892, 0.4684],\n",
      "        [0.8396, 1.0000, 0.1113,  ..., 0.4993, 0.6405, 0.4826],\n",
      "        [0.3640, 0.1113, 1.0000,  ..., 0.8844, 0.7767, 0.3587],\n",
      "        ...,\n",
      "        [0.9660, 0.4993, 0.8844,  ..., 1.0000, 0.7781, 0.7506],\n",
      "        [0.3892, 0.6405, 0.7767,  ..., 0.7781, 1.0000, 0.9799],\n",
      "        [0.4684, 0.4826, 0.3587,  ..., 0.7506, 0.9799, 1.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Construct a random, symmetric matrix of values between 0 and 1\n",
    "D = 4000\n",
    "X = torch.rand((D,D), device=device)\n",
    "X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2dc3036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU HAC parents: [6, 8, 6, 7, 8, 7, 9, 9, 10, 10, 10]\n",
      "Higra HAC parents: [6, 8, 6, 7, 8, 7, 9, 9, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "# Get HAC parents from GPU code\n",
    "_parents = hac_gpu_avg(X, verbose=False, device='cpu')\n",
    "print('GPU HAC parents:', _parents.tolist())\n",
    "\n",
    "# Get HAC parents from Higra\n",
    "_hg_parents = get_parents_from_higra(X, linkage='average')\n",
    "print('Higra HAC parents:', list(_hg_parents))\n",
    "\n",
    "assert np.array_equal(_parents.tolist(),list(_hg_parents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ea817fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test suite\n",
    "\n",
    "for D in range(10,101,10):\n",
    "    X = torch.rand((D,D), device=device)\n",
    "    X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "\n",
    "    # Get HAC parents from GPU code\n",
    "    _parents = hac_gpu_avg(X, verbose=False)\n",
    "#     print('GPU HAC parents:', _parents.tolist())\n",
    "\n",
    "    # Get HAC parents from Higra\n",
    "    _hg_parents = get_parents_from_higra(X, linkage='average')\n",
    "#     print('Higra HAC parents:', list(_hg_parents))\n",
    "\n",
    "    assert np.array_equal(_parents.tolist(), list(_hg_parents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6a323",
   "metadata": {},
   "source": [
    "#### Measure performance difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6396d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.46e+00s\n"
     ]
    }
   ],
   "source": [
    "# Ours: Average-linkage\n",
    "\n",
    "# %%timeit\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "_parents = hac_gpu_avg(X, verbose=False, device='cpu')\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ce257432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.86e+01s\n"
     ]
    }
   ],
   "source": [
    "# Higra: Average-linkage\n",
    "\n",
    "# %%timeit\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "_hg_parents = get_parents_from_higra(X, linkage='average')\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323894d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecac00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a07db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e92f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e6bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "aa6cb98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct hac-cut rounding\n",
    "\n",
    "def avg_hac_cut(X, weights, _MAX_DIST=10, verbose=False, device='cpu', use_similarities=False):\n",
    "    # Initialization\n",
    "    D = X.size(1)\n",
    "    parents = torch.arange(D+(D-1))\n",
    "    parent_to_idx = torch.arange(D+(D-1))\n",
    "    idx_to_parent = torch.arange(D)\n",
    "    cluster_sizes = torch.ones(D+(D-1))\n",
    "    \n",
    "    energy = torch.zeros(D+(D-1), device=device)\n",
    "    clustering = torch.zeros((D+(D-1), D))\n",
    "    clustering[torch.arange(D),torch.arange(D)] = torch.arange(1,D+1, dtype=clustering.dtype)\n",
    "    round_matrix = torch.eye(D, device=device)\n",
    "    \n",
    "    # Take the upper triangular and mask the other values with a large number\n",
    "    Y = _MAX_DIST*torch.ones(D,D, device=device).tril() + (-1 if use_similarities else 1) * X.triu(1)\n",
    "    # Compute the dissimilarity minima per row\n",
    "    values, indices = torch.min(Y, dim=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Initialization:')\n",
    "        print('Y:', Y)\n",
    "        print('\\tparents:', parents)\n",
    "        print('\\tparent_to_idx:', parent_to_idx)\n",
    "        print('\\tidx_to_parent:', idx_to_parent)\n",
    "        print('\\tminima (values):', values)\n",
    "        print('\\tminima (indices):', indices)\n",
    "        print('\\tcluster_sizes:', cluster_sizes)\n",
    "        print()\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    max_node = D-1\n",
    "    if verbose:\n",
    "        print('Starting algorithm:')\n",
    "    for i in range(D-1):\n",
    "        max_node += 1\n",
    "        min_minima_idx = torch.argmin(values).item()\n",
    "\n",
    "        # Merge the index of the minimum value of minimums across rows with the index of the minimum value in its row\n",
    "        merge_idx_1 = min_minima_idx\n",
    "        merge_idx_2 = indices[merge_idx_1].item()\n",
    "\n",
    "        # Find highest-altitude clusters corresponding to the merge indices\n",
    "        parent_1 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_1]]].item()\n",
    "        parent_2 = idx_to_parent[parent_to_idx[idx_to_parent[merge_idx_2]]].item()\n",
    "\n",
    "        if verbose:\n",
    "            print(f'    #{i} Merging:',(merge_idx_1, merge_idx_2),'i.e.', (parent_1, parent_2), '=>', max_node)\n",
    "\n",
    "        # Add parent for the clusters being merged\n",
    "        parents[parent_1] = max_node\n",
    "        parents[parent_2] = max_node\n",
    "\n",
    "        # Update mappings\n",
    "        idx_to_parent[merge_idx_1] = max_node\n",
    "        parent_to_idx[max_node] = merge_idx_1\n",
    "\n",
    "        # Update the matrix with merged values for cluster similarities\n",
    "        max_dist_mask = Y == _MAX_DIST\n",
    "        new_cluster_size = cluster_sizes[parent_1] + cluster_sizes[parent_2]\n",
    "        cluster_sizes[max_node] = new_cluster_size\n",
    "        new_merge_idx_1_values = (torch.min(Y[merge_idx_1, :], Y[:, merge_idx_1]) * cluster_sizes[parent_1] + \\\n",
    "                                  torch.min(Y[:, merge_idx_2], Y[merge_idx_2, :]) * cluster_sizes[parent_2]) / \\\n",
    "                                    new_cluster_size\n",
    "        Y[:, merge_idx_1] = new_merge_idx_1_values\n",
    "        Y[merge_idx_1, :] = new_merge_idx_1_values\n",
    "        Y[max_dist_mask] = _MAX_DIST\n",
    "        Y[:, merge_idx_2] = _MAX_DIST\n",
    "        Y[merge_idx_2, :] = _MAX_DIST\n",
    "\n",
    "        # Update nearest neighbour trackers\n",
    "        values[merge_idx_2] = _MAX_DIST\n",
    "        \n",
    "        max_dist_mask = values == _MAX_DIST\n",
    "        values, indices = torch.min(Y, dim=1)\n",
    "        values[max_dist_mask] = _MAX_DIST\n",
    "        \n",
    "        # Energy calculations\n",
    "        clustering[max_node] = clustering[parent_1] + clustering[parent_2]\n",
    "        leaf_indices = torch.where(clustering[max_node])[0]\n",
    "        leaf_edges = torch.meshgrid(leaf_indices, leaf_indices)\n",
    "        energy[max_node] = energy[parent_1] + energy[parent_2]\n",
    "        merge_energy = torch.sum(weights[leaf_edges])\n",
    "        if merge_energy >= energy[max_node]:\n",
    "            energy[max_node] = merge_energy\n",
    "            clustering[max_node][clustering[max_node] > 0] = max_node\n",
    "            round_matrix[leaf_edges] = 1\n",
    "        \n",
    "        if verbose:\n",
    "            print('Y:', Y)\n",
    "            print('\\tminima (values):', values)\n",
    "            print('\\tminima (indices):', indices)\n",
    "            print('\\tparents:', parents)\n",
    "            print('\\tparent_to_idx:', parent_to_idx)\n",
    "            print('\\tidx_to_parent:', idx_to_parent)\n",
    "            print('\\tcluster_sizes:', cluster_sizes)\n",
    "            print('\\tclustering (current):', clustering[max_node])\n",
    "            print('round_matrix:')\n",
    "            print(round_matrix)\n",
    "            print()\n",
    "    \n",
    "    return round_matrix, clustering[-1], parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee53aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "716ea0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.6269, 0.0217,  ..., 0.8420, 0.4147, 0.8465],\n",
      "        [0.6269, 1.0000, 0.4173,  ..., 0.8729, 0.4535, 0.3830],\n",
      "        [0.0217, 0.4173, 1.0000,  ..., 0.5806, 0.1579, 0.1000],\n",
      "        ...,\n",
      "        [0.8420, 0.8729, 0.5806,  ..., 1.0000, 0.8355, 0.5138],\n",
      "        [0.4147, 0.4535, 0.1579,  ..., 0.8355, 1.0000, 0.5435],\n",
      "        [0.8465, 0.3830, 0.1000,  ..., 0.5138, 0.5435, 1.0000]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0000,  0.4299, -0.2313,  ..., -0.3950,  0.4839,  0.7758],\n",
      "        [ 0.0000,  0.0000, -0.3569,  ...,  0.0936, -0.4037,  0.6479],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0942,  0.3481,  0.7604],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.8543,  0.5286],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.7540],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Construct a random, symmetric matrix of values between 0 and 1\n",
    "D = 100\n",
    "X = torch.rand((D,D), device=device)\n",
    "X = X.triu(1) + X.triu(1).T + torch.eye(D, device=device)\n",
    "print(X)\n",
    "\n",
    "W = torch.rand((D,D), device=device) * 2 - 1\n",
    "W = W.triu(1) + torch.zeros((D,D), device=device)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "625e9e69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.65e-02s\n"
     ]
    }
   ],
   "source": [
    "# Ours: Average-linkage\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "a = time.perf_counter()\n",
    "\n",
    "result = avg_hac_cut(X, W, verbose=False, device='cuda', use_similarities=True)\n",
    "\n",
    "b = time.perf_counter()\n",
    "print('{:.02e}s'.format(b - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4520fcbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(get_parents_from_higra(-X, linkage='average'), result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ae0fc6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 1.,  ..., 1., 1., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 1.,  ..., 1., 1., 0.],\n",
       "         ...,\n",
       "         [1., 0., 1.,  ..., 1., 1., 0.],\n",
       "         [1., 0., 1.,  ..., 1., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0'),\n",
       " tensor([196., 165., 196., 196., 196., 171., 154., 196., 196., 196., 154., 196.,\n",
       "         196., 196., 111., 196., 144., 196., 196., 101.,  21., 196., 196., 135.,\n",
       "         154., 171., 196., 101., 165., 196., 171.,  32., 163., 196., 196., 196.,\n",
       "         196., 163., 121., 111., 121., 160.,  43., 196., 196., 171., 196., 144.,\n",
       "         165., 171., 196., 196., 196., 196., 196., 165., 196., 196., 196., 196.,\n",
       "         165., 170., 196.,  64., 170.,  66.,  67., 105., 196., 196., 170., 163.,\n",
       "         196.,  74., 170., 196., 196.,  78.,  79., 196., 163.,  82., 160., 154.,\n",
       "         196.,  86., 196., 165., 160., 196.,  91., 144., 196., 105.,  95.,  96.,\n",
       "         196., 196., 196., 135.]),\n",
       " tensor([151, 138, 157, 116, 127, 110, 128, 122, 100, 123, 106, 108, 147, 148,\n",
       "         111, 149, 120, 141, 100, 101, 136, 150, 142, 135, 106, 117, 122, 101,\n",
       "         113, 116, 110, 143, 133, 114, 109, 137, 107, 133, 121, 111, 121, 160,\n",
       "         136, 109, 129, 117, 123, 120, 103, 146, 129, 125, 114, 141, 102, 103,\n",
       "         112, 137, 119, 108, 113, 115, 118, 145, 115, 104, 104, 105, 124, 127,\n",
       "         132, 130, 124, 143, 132, 102, 125, 131, 139, 134, 130, 126, 140, 128,\n",
       "         147, 139, 112, 138, 140, 119, 131, 144, 107, 105, 126, 153, 142, 118,\n",
       "         134, 135, 149, 145, 175, 159, 153, 176, 154, 155, 148, 158, 171, 174,\n",
       "         167, 165, 150, 170, 162, 146, 151, 161, 144, 168, 173, 152, 162, 166,\n",
       "         164, 181, 154, 166, 163, 156, 170, 163, 155, 156, 178, 167, 159, 164,\n",
       "         160, 161, 152, 169, 169, 176, 171, 173, 158, 172, 180, 192, 157, 174,\n",
       "         177, 189, 168, 183, 182, 165, 178, 183, 175, 184, 185, 179, 172, 180,\n",
       "         186, 177, 190, 179, 191, 181, 185, 182, 187, 184, 186, 187, 189, 188,\n",
       "         188, 194, 193, 193, 190, 195, 192, 191, 197, 194, 196, 195, 196, 197,\n",
       "         198, 198, 198]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f0eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
